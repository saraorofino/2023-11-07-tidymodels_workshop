---
title: "Intro to `tidymodels`"
author: "Casey O'Hara"
date: "2023-10-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

### Packages you may need to install: tidymodels, ranger (for random forest)

library(tidyverse)
library(here)

### a metapackage like tidyverse, tidymodels contains many model-relevant 
### packages incl rsample, parsnip, recipes, yardstick, broom - we don't 
### need to worry about the differences here...
library(tidymodels) 


```

# Intro

For this workshop/tutorial, we will use the `tidymodels` package to quickly generate and test predictive models, both regression models (predicting a numeric output based on various inputs) and classification models (predicting a categorical output based on various inputs).

This fits squarely within the Tidyverse data science cycle:

![](img/data_cycle.png)

This borrows a lot from the Posit `tidymodels` tutorial that can be found here - visit these pages for more in-depth exploration of the `tidymodels` package:

https://www.tidymodels.org/start/models/

https://www.tidymodels.org/start/recipes/

https://www.tidymodels.org/start/resampling/

https://www.tidymodels.org/start/tuning/

https://www.tidymodels.org/start/case-study/

# Basic model selection with cross validation on a regression task

We want to create a model that best predicts some outcome, based on a set of predictor variables.  Here let's use the built-in diamonds dataset, and try creating a linear model to predict price based on combinations of carat, cut, color, clarity.  Some things we will do:

* from the data create a training set and test set
    * training set for determining values for the model coefficients
    * validation set to see how well the model predicts values from previously unseen data
* create several variations of linear models, using the training set
* use the variations of models to predict price of a diamond based on some set of variables
* create a function to "score" how closely each model predicts price relative to the "known" prices in the validation set
* score each model and see which one most closely predicts.

```{r load data and quick viz}
data(diamonds) ### built into ggplot package
head(diamonds) 

ggplot(diamonds, aes(x = carat, y = price, color = color, shape = cut)) +
  geom_point() +
  theme_minimal()

```


``` {r manual cross validation}

### pseudorandom number generator seed, so we all get the same
### "random" results every time
set.seed(42) 

diamonds_10fold <- diamonds %>% ### dataset built into ggplot package, see data()
  mutate(fold = rep(1:10, length.out = n()),
         fold = sample(fold, size = n(), replace = FALSE)) %>%
  mutate(random = runif(n = n()))

### table(diamonds_10fold$fold)

diamonds_validate <- diamonds_10fold %>%
  filter(fold == 1)
diamonds_train <- diamonds_10fold %>%
  filter(fold != 1)
  
### train on training dataset, saving test data for validation
mdl1 <- lm(price ~ carat + cut + color,           data = diamonds_train)
mdl2 <- lm(price ~ carat + color + clarity,       data = diamonds_train)
mdl3 <- lm(price ~ carat + cut + color + clarity, data = diamonds_train)

### use model to predict values for test dataset
test_df <- diamonds_validate %>%
  mutate(pred1 = predict(mdl1, diamonds_validate),
         pred2 = predict(mdl2, .),  ### note shortcut of `.`
         pred3 = predict(mdl3, .)) %>%
  mutate(resid1 = pred1 - price,
         resid2 = pred2 - price,
         resid3 = pred3 - price)

### Write a scoring function - root-mean-square error
calc_rmse <- function(x) {
  ### x is a vector - square all elements, take mean, then square-root the mean
  sq_error <- x^2 
  mean_sq_error <- mean(sq_error)
  rt_mean_sq_error <- sqrt(mean_sq_error)
  
  return(rt_mean_sq_error)
}

### Compare scores for each model
calc_rmse(test_df$resid1) ### 1467
calc_rmse(test_df$resid2) ### 1177
calc_rmse(test_df$resid3) ### 1166
```

How would we interpret these results?

* Model 1, with price ~ carat + cut + color, did the worst job of predicting (highest RMSE by a long shot)
* Model 2, swapping out cut and replacing with clarity, did a much better job of predicting than model 1.  It seems like clarity is far more important to predicting price than the cut
* Model 3 had the lowest RMSE, so is best of these three models.  Adding cut on top of model 2 modestly improved the predictive performance.
* Play around with other combinations - can you come up with a really bad model?  What happens if you drop carat as a predictor?  why does this make sense?

From here, we could iterate (loop, apply functions, purrr package...) to compare models across each of the various folds.  This is called "K-fold cross validation" - we just did ten-fold cross validation.  Five, ten, or n-fold (leave one out CV) are pretty commonly used.

The automation route would work just fine! but would be a little tedious.

### Thinking through things

Questions to consider:

* How could you use this in other situations?
    * any numeric prediction model, e.g., body mass of penguins, sea surface temperatures, nutrient pollution runoff.
* Why did we train the model on one subset of data and validate it on a different subset?
    * To avoid overfitting - where the model is really good at predicting this exact set of data, but garbage at predicting based on new, previously unseen data.
    * This is important because some more advanced algorithms (e.g., random forest, neural nets) can essentially "memorize" the training data and then appear to perfectly predict it afterward.
* How does this compare to AIC or BIC?
    * xIC methods look at how "likely" the current set of data are with a given model - and add a penalty based on the number of predictors... each additional predictor is guaranteed to improve the likelihood value, but at a risk of overfitting. 


# Tidymodels with a classifier task

Let's use the titanic dataset (the complex one from the `titanic` R package, not the simplified version available in base R).  This dataset contains information on passengers including name, age, ticket number, passenger class, sex... and whether they survived the tragedy or not.  Here we can create a model to predict survival (survived/didn't survive) based on various predictor variables.  Because we're predicting a categorical outcome (survived/didn't), this is a "classification" task.

Consider: what is an example of a categorical outcome you might see in your own research/studies, that would be relevant to a classification task?

* Species threatened status (threatened vs. not threatened)
* Species identity (coastal live oak vs. interior live oak)
* Support for a ballot proposition (likely to vote for vs. against).

## Examining the data

```{r writing out data from titanic package, eval = FALSE}
t_df <- titanic::titanic_train %>%
  janitor::clean_names()
write_csv(t_df, here('data/titanic_survival.csv'))
```

```{r load titanic data}
t_df <- read_csv(here('data/titanic_survival.csv'))
### View(t_df) ### examine the data

### Let's do a little processing to create a "survival" dataframe
surv_df <- t_df %>%
  mutate(survived = factor(survived),   ### categorical outcome variables need to be factors
         pclass   = factor(pclass)) %>% ### turn some predictors to factor
  select(-cabin, -ticket) ### lots of NAs here - and not likely to be very helpful

### exploratory plots: try sex, fare, etc
ggplot(surv_df, aes(x = pclass, fill = survived)) +
  geom_bar()

ggplot(surv_df, aes(x = age, fill = survived)) +
  geom_histogram()
```

Which predictor variables seem like they might be good at predicting survival (TRUE vs FALSE)?

# Using `tidymodels`

### Split the data

We will set aside ("partition") a portion of the data for building and comparing our models (80%), and a portion for training our models after we've selected the best one (20%).  NOT the same as folds - that will happen in the training/validation step.

```{r split the data}
### Check balance of survived column
surv_df %>%
  group_by(survived) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(prop = n / sum(n))
### if very unbalanced, choose a stratified split to make sure there are enough
### survivors in the test and training splits.

set.seed(123)

surv_split <- initial_split(surv_df, prop = 0.80, strata = survived)
  ### stratified on `survived`; training and test splits will both have ~60/40% survived = 0/1
surv_train_df <- training(surv_split)
surv_test_df <- testing(surv_split)
```

## `tidymodels`: Basic model with `parsnip`

We can set up a basic logistic regression model using functions from the `parsnip` package, which contains a bunch of different model types, and links to multiple model engines for each type (e.g., there are multiple R packages that can calculate a linear model).  The `parsnip` package consolidates a lot of these and helps make the parameters, arguments, and results consistent.

We'll use a binary logistic regression, which predicts the probability (technically, log odds) of outcome A vs. outcome B (e.g., survived vs. did not survive) based on a linear combination of predictors (e.g., passenger class, sex, fare).

```{r set up a binary logistic regression model with our data}
blr_mdl <- logistic_reg() %>%
  set_engine('glm') ### this is the default - we could try engines from other packages or functions

blr_fit <- blr_mdl %>%
  fit(survived ~ sex + pclass, data = surv_train_df)

### let's also create a model we know will be bad:
garbage_fit <- blr_mdl %>%
  fit(survived ~ passenger_id + embarked, data = surv_train_df)

blr_fit
```

Males are far less likely to survive than reference class female (negative means lower odds which translates to lower probability); passenger classes 2 and 3 are also less likely to survive than reference class 1.

How well does this model predict survival of the test dataset?  Let's use our fitted model from the training set on the test set, and create a confusion matrix to see how well the predictions line up.

```{r}
surv_test_predict <- surv_test_df %>%
  ### straight up prediction, based on 50% prob threshold (to .pred_class):
  mutate(predict(blr_fit, new_data = surv_test_df)) %>%
  ### but can also get the raw probabilities of class A vs B (.pred_A, .pred_B):
  mutate(predict(blr_fit, new_data = ., type = 'prob'))
    ### note use of `.` as shortcut for "the current dataframe"

table(surv_test_predict %>%
        select(survived, .pred_class))

#         .pred_class
# survived  0  1
#        0 93 17
#        1 17 52
```

Try it with a new formula above, can you find a model that improves accuracy?  How much worse is a model based only on sex or only on passenger class?

Metrics: we can use metrics from the `yardstick` package (within `tidymodels`) to test accuracy and the Receiver Operating Characteristic curve...
```{r}
accuracy(surv_test_predict, truth = survived, estimate = .pred_class)
```

![from https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/](img/roc-curve-v2.png)
```{r}
roc_df <- roc_curve(surv_test_predict, truth = survived, .pred_0)
autoplot(roc_df)

### how about our garbage model?
garbage_test_df <- surv_test_df %>%
  mutate(predict(garbage_fit, new_data = ., type = 'prob')) 

garbage_roc_df <- garbage_test_df %>%
  roc_curve(truth = survived, .pred_0) 

autoplot(garbage_roc_df)

### Calculate area under curve - 50% is random guessing, 100% is perfect classifier
yardstick::roc_auc(surv_test_predict, truth = survived, .pred_0)
yardstick::roc_auc(garbage_test_df, truth = survived, .pred_0)
```

### So what?

We basically could have done all that the old way... why would I want to use `tidymodels`?

* `parsnip` standardizes different models and engines from across a wide range of packages and algorithms - we can easily change the binary logistic regression engine to a different package without having to change anything else in our code, or even an entirely different model (e.g., random forest) with minimal changes.
* `tidymodels` also includes other features for more advanced model creation and cross validation.

## `tidymodels`: Cross validation!

We can take our `surv_train_df` and split it out into folds using functions from `rsample` (another part of `tidymodels`):

```{r}
set.seed(10101)
surv_train_folds <- vfold_cv(surv_train_df, v = 10)
surv_train_folds
```

Automates that first step we did!

Now let's create a `workflow` that combines our model and a formula.  We already specified a binary logistic regression model above.  The workflow specifies how R will operate across all the folds.
```{r}
# blr_mdl <- logistic_reg() %>%
#   set_engine('glm') ### this is the default - we could try engines from other packages or functions

blr_wf <- workflow() %>%   ### initialize workflow
  add_model(blr_mdl) %>%
  add_formula(survived ~ pclass + sex)
  # add_formula(survived ~ pclass + sex + fare)
```

OK now let's apply the workflow to our folded training dataset, and see how it performs!

```{r}
blr_fit_folds <- blr_wf %>%
  fit_resamples(surv_train_folds)

blr_fit_folds

### Average the predictive performance of the ten models:
collect_metrics(blr_fit_folds)
```

With this workflow setup, we can change the formula and rerun the entire process easily to compare different variations on our model.

### let's switch up the model, let's try random forest!

```{r}
rf_mdl <- rand_forest(trees = 1000) %>%
  set_engine('ranger') %>% ### this is the default - other engines available
  set_mode('classification') ### RF can do classification OR regression; need to specify!

rf_wf <- workflow() %>%   ### initialize workflow
  add_model(rf_mdl) %>%
  add_formula(survived ~ pclass + sex)
  # add_formula(survived ~ pclass + sex + fare)
```

OK now let's apply the workflow to our folded training dataset, and see how it performs!

```{r}
rf_fit_folds <- rf_wf %>%
  fit_resamples(surv_train_folds)

rf_fit_folds

### Average the predictive performance of the ten models:
collect_metrics(rf_fit_folds)
```

## Last fit!

We tried a logistic regression and random forest model on our training data, using cross validation to resample the training data and see how each model performed.  With the fare included as a predictor, the random forest modestly outperformed the logistic regression in the crossvalidation step.

```{r}
last_rf_fit <- rf_wf %>%
  last_fit(surv_split)
collect_metrics(last_rf_fit)
```


# `tidymodels`: Other fancy stuff

Other things you could explore with tidymodels: 

* `recipes` to pre-process your data especially for more complex datasets
* Tuning model hyperparameters - e.g., for random forest, how many decision trees, how many predictors per tree, tree max depth; for neural networks, how many hidden units, etc.

